{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0621b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:46:33.738448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 01:46:33.886616: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:46:33.886646: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-23 01:46:33.921392: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 01:46:34.739388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:46:34.739561: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 01:46:34.739579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# means that my matplotlib graphs will be included in the notebook, next to the code\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os \n",
    "\n",
    "# Imports added from modifying the original code\n",
    "import random\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, Column, join\n",
    "from hetdex_tools.get_spec import get_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1188f3",
   "metadata": {},
   "source": [
    "Keras is a deep learning API written in Python, running on top of ther machine learning platform TensorFlow. Keras was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "So Keras is a high-level neural network library that runs on top of TensorFlow. Keras is more user friendly because it's built in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import History "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805fe91",
   "metadata": {},
   "source": [
    "Comments and notes by me (Nick D) :) just to help me understand what's going on.\n",
    "\n",
    "Callbacks: A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)\n",
    "\n",
    "Callbacks can be used to:\n",
    "   * Monitor metrics by writing TensorBoard logs\n",
    "   * Periodically save model to disk drive\n",
    "   * Do early stopping\n",
    "   * Get internal states and stats during training\n",
    "   * and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085044a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is creating a 'History' callback object. This object keeps track of the accuracy, loss and other training metrics\n",
    "# for each epoch in the memory.\n",
    "# Not 100% sure, but it seems like this is what allows the code to make plots later on.\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not entirely sure what the number means. Must be a naming convention about what her cuts were in the data\n",
    "# after she normalized\n",
    "#training_index = \"31_cut\"\n",
    "training_index = \"nick_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656d86",
   "metadata": {},
   "source": [
    "### Opening HDR3 Catalog and converting it into a Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening HDR3 detections catalog and converting it into a pandas DF\n",
    "# HDR3 is detections HETDEX found\n",
    "# Using HDR3 NEP and not H20 NEP because we can make SN cuts!\n",
    "HDR_source_cat = fits.open('/home/jovyan/Hobby-Eberly-Telesco/hdr3/catalogs/source_catalog_3.0.1.fits', memmap = True)\n",
    "HDR3_data = HDR_source_cat[1].data\n",
    "HDR3_DF = pd.DataFrame(HDR3_data, columns=HDR3_data.columns.names)\n",
    "\n",
    "# Columns we will then take from the entire data set (it was huge so we needed to determine what we wanted to look at specifically).\n",
    "# As the name suggests, these are the ones that are useful to us!\n",
    "useful_hdr3_cols = ['source_id', 'detectid',  'selected_det', 'ra_mean', 'dec_mean', 'fwhm', 'shotid', 'field',  'ra', 'dec', 'wave', 'wave_err', 'flux', 'flux_err', 'sn', 'sn_err', 'chi2', 'chi2_err',\n",
    "'linewidth', 'linewidth_err', 'plya_classification', 'z_hetdex', 'z_hetdex_conf', 'combined_plae']\n",
    "\n",
    "# From the original DFs, taking the useful columns\n",
    "reduced_hdr3_df = HDR3_DF.loc[:, useful_hdr3_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbb5b2",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data from before 2017 because it isn't good (not useful to us)\n",
    "# No need to do this for H20 NEP\n",
    "removed_bad_shots_hdr3_df = reduced_hdr3_df[reduced_hdr3_df.shotid.values >= 20180000000]\n",
    "\n",
    "# This will give high confidence detections. Something we would want to do also. What is sn threshold that Valentina's code is having trouble with.\n",
    "# Reason why, we want high-confidence Lya. If we are very confident sn and another filter, then that's what we consider high-conf lya.\n",
    "# Once noise and high-confidence sample. We can start exanping on valentina's code and do our own stuff\n",
    "# We can start with high SN > 6, and slowly decrease to SN > 5\n",
    "hdr3_signal_to_noise_interval = removed_bad_shots_hdr3_df[removed_bad_shots_hdr3_df['sn'] > 6]\n",
    "\n",
    "\n",
    "# Now I just take the sources stricly in the NEP field\n",
    "hdr3_nep = hdr3_signal_to_noise_interval[hdr3_signal_to_noise_interval['field'] == 'nep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ff09a",
   "metadata": {},
   "source": [
    "Making skycoord object for hdr3 ra and dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr3_skycoords = SkyCoord(hdr3_nep['ra'] * u.deg, hdr3_nep['dec'] * u.deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8902f6",
   "metadata": {},
   "source": [
    "# Oscar's plan\n",
    "### (Saving cause slack deletes messages sometimes)\n",
    "Oh I do not think you want to input your noise sample through Valentina’s code, cause I do not think Valentina trained her encoder with that. What we would want to do is take a random sample of sources in the NEP field, run it through Valentina’s code and then select the output that her code says are high-z. Then we would run this “high-z” sample through your noise separator classification.\n",
    "So it’ll be a two step process where you would test and train your ML algorithm using random forrest on LAEs and noise, optimize it so it has the highest accuracy possible.\n",
    "Once you are comfortable with your ML algorithm then we do what i mentioned above, where we take some random sources, maybe we could apply some cuts to enhance the sample, in the NEP field run it through Valentina’s code select what her code says are high-z sources and then run those high-z sources through your ML algorithm and see which of those are actual LAEs and which are noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c8db2",
   "metadata": {},
   "source": [
    "# QUESTIONS:\n",
    "1. Logging into TACC in public wifi? [x]\n",
    "\n",
    "Yes!\n",
    "\n",
    "2.  Is it ok to only use single spectras, or is that like not realistic data anymore? When I didn't filter for only 1 spectra, the shape of my array got weird and it didn't work with a lot of other things in the code.\n",
    "\n",
    "# To DO:\n",
    "\n",
    "1. Write code for checking for duplicates\n",
    "    - Use wavelength to check, if its within a couple angstroms of each other.\n",
    "    - Could use detectID as well.\n",
    "    \n",
    "2. Look into: \"Why do we need a test and train set?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df071e67",
   "metadata": {},
   "source": [
    "### TAKES A LONG TIME. SKIP IF DONT HAVE TIME (have a saved version below) Extracts a random sample of sources in NEP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68806c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0,1,2,3,2,4,5,1,7,8,9,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f70062",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_arr = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d7eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique returns the sorted unique elements of an array\n",
    "# 'return_counts' return the number of times each unique item appears in input array\n",
    "# in this case the input array is random_nep_shotids\n",
    "# unique_return is the sorted unique elements, not really useful to us here\n",
    "unique_return, count = np.unique(arr, return_counts = True)\n",
    "\n",
    "# Find which number appears more than once\n",
    "value_repeated = np.where(count > 1)[0]\n",
    "\n",
    "# The np.where function returns a tuple in this case but with only a single value\n",
    "# So I added the 0 at the end to only give me the value\n",
    "# Use np.where to give me the index where the inverse (basically the normal array) equals the repeated value\n",
    "repeated_indeces = (np.where(arr == value_repeated))[0]\n",
    "\n",
    "# If something repeats, then remove those indeces from the spectra array!\n",
    "if value_repeated.size >= 1:\n",
    "    del_arr = np.delete(del_arr, repeated_indeces[1:])\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0c7cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  2,  4,  5,  1,  7,  8,  9, 10,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5a1375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  2,  4,  5,  1,  7,  8,  9, 10,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83bc9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inverse, count = np.unique(arr, return_inverse=True,\n",
    "                              return_counts=True)\n",
    "\n",
    "# Find which number appears more than once\n",
    "value_repeated = np.where(count > 1)[0]\n",
    "\n",
    "# Use np.where to give me the index where the inverse (basically the normal array) equals the repeated value\n",
    "# The np.where function returns a tuple in this case but with only a single value\n",
    "# So I added the 0 at the end to only give me the value\n",
    "repeated_indeces = (np.where(inverse == value_repeated))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_arr, count = np.unique(arr, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6313dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_unique_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bde061",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07569777",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_repeated = np.where(count > 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9234acd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_repeated.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b522b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  2,  4,  5,  1,  7,  8,  9, 10,  1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbaf4bcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_vals_repeated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43midx_vals_repeated\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_vals_repeated' is not defined"
     ]
    }
   ],
   "source": [
    "idx_vals_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e166c795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09791a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all repeated indeces except the first ('original') one.\n",
    "test2 = np.delete(arr, repeated_indeces[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b03575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999d70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make it 1300, will take a very long time to run!\n",
    "# making it 1300 because I will later check for duplicates! Want to make sure I have some wiggle room and don't go under 1000\n",
    "    # Use wavelength to check, if its within a couple angstroms of each other.\n",
    "    # Could use detectID as well.\n",
    "samples_amount = 10\n",
    "\n",
    "# Initialize my 2d list which will later be turned into an array\n",
    "# the range is just how many columns (I THINK DOUBLE CHECK THIS)\n",
    "random_nep_LS = [[0] * 1 for i in range(samples_amount)]\n",
    "\n",
    "shotid_LS = [0 * 1 for i in range(samples_amount)]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < samples_amount:\n",
    "    #pick a random coordinate from the skycoord object\n",
    "    random_coord = hdr3_skycoords[random.randint(0, hdr3_skycoords.size)]\n",
    "    # extract spectra at this random coordinate and append to list, can't append to np array because of \n",
    "    # data type issues will convert to array after loop!\n",
    "    extraction = get_spectra(random_coord)\n",
    "    \n",
    "    if len(extraction) == 1:\n",
    "             \n",
    "        squeezed_extraction = np.squeeze(extraction['spec'].value)\n",
    "        random_nep_LS[counter] = squeezed_extraction\n",
    "        shotid_LS[counter] = extraction['shotid'].value\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "random_nep_spectra = np.array(random_nep_LS)\n",
    "random_nep_shotids = np.array(shotid_LS)\n",
    "\n",
    "# np.unique returns the sorted unique elements of an array\n",
    "# if you use the optional 'return_counts' return the number of times each unique item appears in input array\n",
    "# in this case the input array is random_nep_shotids\n",
    "# unique_return is the sorted unique elements, not really useful to us here\n",
    "unique_return, count = np.unique(random_nep_shotids, return_counts = True)\n",
    "\n",
    "# Find which number appears more than once\n",
    "value_repeated = np.where(count > 1)[0]\n",
    "\n",
    "# The np.where function returns a tuple in this case but with only a single value\n",
    "# So I added the 0 at the end to only give me the value\n",
    "# Use np.where to give me the index where the inverse (basically the normal array) equals the repeated value\n",
    "repeated_indeces = (np.where(inverse == value_repeated))[0]\n",
    "\n",
    "# If something repeats, then remove those indeces from the spectra array!\n",
    "if value_repeated.size >= 1:\n",
    "    np.delete(random_nep_spectra, repeated_indeces[1:])\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b3d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eeb094",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_nep_spectra[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e01799",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_nep_shotids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418be157",
   "metadata": {},
   "source": [
    "## Notes on trying to find duplicates so I can remove them.\n",
    "\n",
    "Since spectrum extraction returns an astropy table, my first thought was to make a list of astropy tables and either make a large astropy table or a dataframe. I opted with the dataframe since it has more functionality to remove duplicates and filter things. However the extraction tables have some parts which are multidimensional, and .to_pandas doesn't allow for that. So now my plan is to use the same logic I used to store the actual spectrum numbers/values to store the shotid of everything. Then the shotids and exctraction value's indeces should match. I can then just check if I have any repeating shotid's. If the shot id's match anywhere, then we remove those shot id's at their indeces and remove the spectra values at those indeces as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324f56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make it 1300, will take a very long time to run!\n",
    "# making it 1300 because I will later check for duplicates! Want to make sure I have some wiggle room and don't go under 1000\n",
    "    # Use wavelength to check, if its within a couple angstroms of each other.\n",
    "    # Could use detectID as well.\n",
    "samples_amount = 10\n",
    "\n",
    "# Initialize my 2d list which will later be turned into an array\n",
    "# the range is just how many columns (I THINK DOUBLE CHECK THIS)\n",
    "random_nep_LS = [[0] * 1 for i in range(samples_amount)]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < samples_amount:\n",
    "    #pick a random coordinate from the skycoord object\n",
    "    random_coord = hdr3_skycoords[random.randint(0, hdr3_skycoords.size)]\n",
    "    # extract spectra at this random coordinate and append to list, can't append to np array because of \n",
    "    # data type issues will convert to array after loop!\n",
    "    extraction = get_spectra(random_coord)\n",
    "    \n",
    "    if len(extraction) == 1: \n",
    "        squeezed_extraction = np.squeeze(extraction['spec'].value)\n",
    "        random_nep_LS[counter] = squeezed_extraction\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "random_nep_spectra = np.array(random_nep_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be567602",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702336ec",
   "metadata": {},
   "source": [
    "# Check for duplicates before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93728393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_nep_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad25495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(f\"random_nep_sample_{samples_amount}\", random_nep_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in spectra sample\n",
    "# The allow pickle part uses the pickle module which is not secure against maliciously constructed data\n",
    "# Don't allow pickle if you don't trust the source, I do so it doesn't matter to me\n",
    "# I have to do the allow_pickle = True for the code to not give me an error anyway\n",
    "NEP_spec = np.load(\"random_nep_sample_1300.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEP_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdf9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in spectra samples\n",
    "star_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/star_spec_reduced.npy\") \n",
    "agn_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/all_agn_no_duplicates.npy\")\n",
    "lowz_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/lowz_no_duplicates.npy\")\n",
    "gal_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/gal_sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks for nan values.\n",
    "# Interestingly enough, when you do nan != nan, you get True. Becuase NaN is unequal with anything.\n",
    "# But with numbers, you'll get false.\n",
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b74e5",
   "metadata": {},
   "source": [
    "### Could also use numpy isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73500a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any values in the imports that are 'nan' (Not a Number) we remove them.\n",
    "# Go through each value in matrix and check if NaN\n",
    "def remove_nan(file):\n",
    "    for i in range(0,len(file)):\n",
    "        for j in range(0,len(file[i])):\n",
    "            if isNaN(file[i][j]):\n",
    "\n",
    "                # if value is nan, make the value equal to 0.0001\n",
    "                # I'm assuming this was to avoid errors\n",
    "                file[i][j]=0.00001\n",
    "\n",
    "# remove_nan(star_spec)\n",
    "# remove_nan(agn_spec)\n",
    "# remove_nan(lowz_spec)\n",
    "# remove_nan(gal_spec)\n",
    "# Adding NEP files to the remove NaN function calls\n",
    "remove_nan(NEP_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(2) will make the random numbers predictable, so random numbers, but the same random numbers from the same set/seed\n",
    "# so I'll get the same random numbers everytime, until I change the seed\n",
    "np.random.seed(2)\n",
    "# random.shuffle will shuffle the values in an array or list. Like shuffling a deck of cards\n",
    "# I wonder why she only shuffled starspec.\n",
    "\n",
    "# np.random.shuffle(star_spec)\n",
    "np.random.shuffle(NEP_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fcf7c",
   "metadata": {},
   "source": [
    "Tying to split between testing and training belo. scikit learning has something to make randomize data set and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate just joins arrays\n",
    "all_images = np.concatenate((star_spec[::90],agn_spec,lowz_spec,gal_spec))\n",
    "# making a training set\n",
    "train_images = np.concatenate((star_spec[::90][:433],agn_spec[:444],lowz_spec[:837],gal_spec[:254]))\n",
    "#train_images = agn_spec[:433]\n",
    "#np.random.shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa40258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure what these val_images are, it seems to be the opposite half of the train_images.\n",
    "val_images = np.concatenate((star_spec[::90][433:],agn_spec[444:],lowz_spec[837:],gal_spec[254:]))\n",
    "#val_images = agn_spec[433:]\n",
    "#np.random.shuffle(val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f56f6",
   "metadata": {},
   "source": [
    "Largest trunk is training, validation is smaller, and test is smallest! Most places I looked Training much bigger by a decent margin. So I'm going from Training: 50, Valid: 30, Test: 20 to Training: 60, Valid: 30, Test = 10\n",
    "\n",
    "Training = 60\n",
    "\n",
    "validation = 30\n",
    "\n",
    "test = 10\n",
    "\n",
    "Main difference only training on training set. Others are to test how well it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4d7ff",
   "metadata": {},
   "source": [
    "# Definition of Train-Valid-Test Split\n",
    "(https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c)\n",
    "Train-Valid-Test split is a technique to evaluate the performance of your machine learning model — classification or regression alike. You take a given dataset and divide it into three subsets. A brief description of the role of each of these datasets is below.\n",
    "\n",
    "## Train Dataset\n",
    "Set of data used for learning (by the model), that is, to fit the parameters to the machine learning model\n",
    "## Valid Dataset\n",
    "Set of data used to provide an unbiased evaluation of a model fitted on the training dataset while tuning model hyperparameters.\n",
    "Also play a role in other forms of model preparation, such as feature selection, threshold cut-off selection.\n",
    "## Test Dataset\n",
    "Set of data used to provide an unbiased evaluation of a final model fitted on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759e3e8",
   "metadata": {},
   "source": [
    "### My splitting between training, validation. and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37559e",
   "metadata": {},
   "source": [
    "## Below this is code that splits the data into training, validation and testing. However Valentina's code only uses training and validation. So I will do that as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here train_test_split takes in an array (NEP_spec)\n",
    "# a test_size which is just a float from 0 to 1 to tell it how to split up the train and tests\n",
    "# and random_state which works similar to the .shuffle. It helps control the shuffling and helps us reproduce outputs\n",
    "# imported from\n",
    "\n",
    "# Want to split the data into 60:30:10 for train:valid:test dataset\n",
    "# If we use sklearn then this takes 3 steps, I found a package called fast_ml\n",
    "# that did this in one step, but felt that using sklearn was better since it was more popular\n",
    "# and so I could get help easier if something went wrong\n",
    "\n",
    "# First we split the data into training, and a remaining dataset\n",
    "#NEP_train, NEP_valid = train_test_split(NEP_spec, test_size = 0.4, random_state = 42)\n",
    "\n",
    "# Now since we want test to be 10% of overall data, so I take 25% of the NEP_valid (1/4 of 40 = 10)\n",
    "#NEP_valid, NEP_test = train_test_split(NEP_valid, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# print(\"NEP_train.shape\", NEP_train.shape)\n",
    "# print(\"NEP_valid.shape\", NEP_valid.shape)\n",
    "# print(\"NEP_test.shape\", NEP_test.shape)\n",
    "\n",
    "\n",
    "#NEP_train, NEP_valid, NEP_test = train_valid_test_split(NEP_spec, train_size = 0.6, valid_size = 0.3, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02309fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valentina's validation is 1/4th the size of her traning, so I will do that as well\n",
    "# and make my training 75% of the whole data set.\n",
    "NEP_train, NEP_valid = train_test_split(NEP_spec, train_size = 0.75, random_state = 42)\n",
    "\n",
    "print(\"NEP_train.shape\", NEP_train.shape)\n",
    "print(\"NEP_valid.shape\", NEP_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will normalize the data.\n",
    "# The idea is, there is such a large range of sources so it may be simpler for the algorithm to recognize sources\n",
    "# if they are between 0 and 1.\n",
    "# Dividing by the largest number preserves the information but scales it down.\n",
    "# If we don't do this the sources could go from a very large number to a small one, which makes it harder for the algorithm\n",
    "def Norm(data):\n",
    "    new_data = []\n",
    "    for spec in data:\n",
    "        #print('spec_before', spec)\n",
    "        spec = spec/spec.max()\n",
    "        #print('spec_after', spec)\n",
    "        #print('spec.max', spec.max())\n",
    "        new_data.append(spec)\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing both data sets\n",
    "# train_images = Norm(train_images)\n",
    "# val_images = Norm(val_images)\n",
    "\n",
    "NEP_train = Norm(NEP_train)\n",
    "NEP_valid = Norm(NEP_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51cfd7",
   "metadata": {},
   "source": [
    "Cutting out left and right ends, because when making inspection there can be weird things on the left most end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c794651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting out left and right ends, because when making inspection there can be weird things on the left most end.\n",
    "# As of right now, not sure if I need to cut anything.\n",
    "# Making cuts of the image sets and then normalizing them.\n",
    "train_cut = train_images[:,88:1002]\n",
    "val_cut = val_images[:,88:1002]\n",
    "train_cut = Norm(train_cut)\n",
    "val_cut = Norm(val_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b19cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 30\n",
    "act = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "          #layers.InputLayer(input_shape=(1036)),\n",
    "          layers.Dense(436, activation=act,name=\"e1\"),\n",
    "          #layers.Dropout(0.3),\n",
    "          #layers.Dense(136, activation=act, name ='e2'),\n",
    "          #BatchNormalization(),\n",
    "          layers.Dropout(0.3),\n",
    "          layers.Dense(latent_dim, activation=None, name = \"e3\"),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "          #layers.InputLayer(input_shape=(latent_dim,)),\n",
    "          #layers.Dense(136, activation=act,name = \"d1\"),\n",
    "          layers.Dense(436, activation=act,name = \"d2\"),\n",
    "          #BatchNormalization(),\n",
    "          layers.Dense(1036, activation=None,name = \"d3\")\n",
    "        ])#914 for cut. 1036 for all\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a658af",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_dim)\n",
    "#autoencoder.build(input_shape=np.shape(train_images))\n",
    "epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adg = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    d = tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)\n",
    "    #tf.print(d, [d], \"Inside loss function\")\n",
    "    #tf.print(d.shape)\n",
    "    return d\n",
    "\n",
    "autoencoder.compile(optimizer=adam, loss=loss1)#es.MeanSquaredError())\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"models/training{training_index}\")\n",
    "    checkpoint_path = f\"models/training{training_index}/cp_{training_index}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "except:\n",
    "    checkpoint_path = f\"models/training{training_index}/cp_{training_index}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL FROM VALENTINA'S CODE!!\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "#autoencoder.summary()\n",
    "#autoencoder.fit(train_images, train_images,\n",
    " #               epochs=10,\n",
    "  #              shuffle=True,\n",
    "   #             validation_data=(val_images,val_images))\n",
    "hist = autoencoder.fit(train_cut, train_cut,\n",
    "                epochs=epoch,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_cut,val_cut),\n",
    "                callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEP_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be98d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEP_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "#autoencoder.summary()\n",
    "#autoencoder.fit(train_images, train_images,\n",
    " #               epochs=10,\n",
    "  #              shuffle=True,\n",
    "   #             validation_data=(val_images,val_images))\n",
    "hist = autoencoder.fit(NEP_train, NEP_train,\n",
    "                epochs=epoch,\n",
    "                shuffle=True,\n",
    "                validation_data=(NEP_valid,NEP_valid),\n",
    "                callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b415d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.linspace(1,epoch+1,epoch)\n",
    "plt.plot(x,hist.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(x,hist.history[\"val_loss\"],label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'Graphs/loss_{training_index}.png')\n",
    "#print(np.shape(train_images))\n",
    "\n",
    "encoded_imgs = autoencoder.encoder(NEP_train).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "np.save(f\"encoded_{training_index}\",encoded_imgs)\n",
    "np.save(f\"decoded_{training_index}\",decoded_imgs)\n",
    "\n",
    "encoded_val = autoencoder.encoder(NEP_valid).numpy()\n",
    "np.save(f\"encoded_val_{training_index}\",encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b73022",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"encoded_31_cut.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(NEP_train[4])\n",
    "plt.savefig(\"OG.png\")\n",
    "plt.figure()\n",
    "plt.plot(decoded_imgs[4])\n",
    "plt.savefig(\"RECON.png\")\n",
    "print(hist.history[\"val_loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fb812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
