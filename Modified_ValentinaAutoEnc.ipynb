{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0621b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os \n",
    "\n",
    "# Imports added from modifying the original code\n",
    "import random\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astropy.table import Table, Column, join\n",
    "from hetdex_tools.get_spec import get_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1188f3",
   "metadata": {},
   "source": [
    "Keras is a deep learning API written in Python, running on top of ther machine learning platform TensorFlow. Keras was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "So Keras is a high-level neural network library that runs on top of TensorFlow. Keras is more user friendly because it's built in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c7e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import History "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805fe91",
   "metadata": {},
   "source": [
    "Comments and notes by me (Nick D) :) just to help me understand what's going on.\n",
    "\n",
    "Callbacks: A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)\n",
    "\n",
    "Callbacks can be used to:\n",
    "   * Monitor metrics by writing TensorBoard logs\n",
    "   * Periodically save model to disk drive\n",
    "   * Do early stopping\n",
    "   * Get internal states and stats during training\n",
    "   * and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "085044a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is creating a 'History' callback object. This object keeps track of the accuracy, loss and other training metrics\n",
    "# for each epoch in the memory.\n",
    "# Not 100% sure, but it seems like this is what allows the code to make plots later on.\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bd6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not entirely sure what the number means. But I'm assuming it's purely a naming convention and does not\n",
    "# mean anything specific.\n",
    "training_index = \"31_cut\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656d86",
   "metadata": {},
   "source": [
    "### Opening HDR3 Catalog and converting it into a Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08d082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening HDR3 detections catalog and converting it into a pandas DF\n",
    "# HDR3 is detections HETDEX found\n",
    "# Using HDR3 NEP and not H20 NEP because we can make SN cuts!\n",
    "HDR_source_cat = fits.open('/home/jovyan/Hobby-Eberly-Telesco/hdr3/catalogs/source_catalog_3.0.1.fits', memmap = True)\n",
    "HDR3_data = HDR_source_cat[1].data\n",
    "HDR3_DF = pd.DataFrame(HDR3_data, columns=HDR3_data.columns.names)\n",
    "\n",
    "# Columns we will then take from the entire data set (it was huge so we needed to determine what we wanted to look at specifically).\n",
    "# As the name suggests, these are the ones that are useful to us!\n",
    "useful_hdr3_cols = ['source_id', 'detectid',  'selected_det', 'ra_mean', 'dec_mean', 'fwhm', 'shotid', 'field',  'ra', 'dec', 'wave', 'wave_err', 'flux', 'flux_err', 'sn', 'sn_err', 'chi2', 'chi2_err',\n",
    "'linewidth', 'linewidth_err', 'plya_classification', 'z_hetdex', 'z_hetdex_conf', 'combined_plae']\n",
    "\n",
    "# From the original DFs, taking the useful columns\n",
    "reduced_hdr3_df = HDR3_DF.loc[:, useful_hdr3_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbb5b2",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f35cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data from before 2017 because it isn't good (not useful to us)\n",
    "# No need to do this for H20 NEP\n",
    "removed_bad_shots_hdr3_df = reduced_hdr3_df[reduced_hdr3_df.shotid.values >= 20180000000]\n",
    "\n",
    "# This will give high confidence detections. Something we would want to do also. What is sn threshold that Valentina's code is having trouble with.\n",
    "# Reason why, we want high-confidence Lya. If we are very confident sn and another filter, then that's what we consider high-conf lya.\n",
    "# Once noise and high-confidence sample. We can start exanping on valentina's code and do our own stuff\n",
    "hdr3_signal_to_noise_interval = removed_bad_shots_hdr3_df[removed_bad_shots_hdr3_df['sn'] > 6]\n",
    "\n",
    "\n",
    "# Now I just take the sources stricly in the NEP field\n",
    "hdr3_nep = hdr3_signal_to_noise_interval[hdr3_signal_to_noise_interval['field'] == 'nep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ff09a",
   "metadata": {},
   "source": [
    "Making skycoord object for hdr3 ra and dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1214e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr3_skycoords = SkyCoord(hdr3_nep['ra'] * u.deg, hdr3_nep['dec'] * u.deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c8db2",
   "metadata": {},
   "source": [
    "# QUESTIONS:\n",
    "1. Would I have to check for duplicates before extracting? []\n",
    "\n",
    "2. Do I care about spec_err for now? For now, no. [x]\n",
    "\n",
    "3. How big should my random sample be? []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63750283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0105d5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de68cdc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO - 2022-11-11 05:59:30,570] Finding shots of interest\n",
      "[INFO - 2022-11-11 05:59:36,733] Number of shots of interest: 8\n",
      "[INFO - 2022-11-11 05:59:36,735] Extracting 8 sources\n",
      "[INFO - 2022-11-11 05:59:36,922] Working on shot: 20210510026\n",
      "[INFO - 2022-11-11 05:59:37,006] Working on shot: 20210605015\n",
      "[INFO - 2022-11-11 05:59:37,018] Working on shot: 20210608014\n",
      "[INFO - 2022-11-11 05:59:37,079] Working on shot: 20210610021\n",
      "[INFO - 2022-11-11 05:59:37,118] Working on shot: 20210611014\n",
      "[INFO - 2022-11-11 05:59:37,177] Working on shot: 20210612015\n",
      "[INFO - 2022-11-11 05:59:37,250] Working on shot: 20210616020\n",
      "[INFO - 2022-11-11 05:59:37,299] Working on shot: 20210616021\n",
      "[INFO - 2022-11-11 05:59:38,716] Extracting 1\n",
      "[INFO - 2022-11-11 05:59:39,204] Extraction of sources completed in 0.04 minutes.\n",
      "[INFO - 2022-11-11 05:59:39,333] Retrieved 1 spectra.\n",
      "[INFO - 2022-11-11 05:59:39,424] Finding shots of interest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction['spec'].value [[ 9.00642944  9.01742971  9.02839659 ... 12.92817222 12.93024977\n",
      "  12.93230585]]\n",
      "extraction['spec'].value.shape (1, 1036)\n",
      "x [[ 9.00642944  9.01742971  9.02839659 ... 12.92817222 12.93024977\n",
      "  12.93230585]]\n",
      "x[0].shape (1036,)\n",
      "x.shape (1, 1036)\n",
      "np.squeeze(x, axis = 0) [ 9.00642944  9.01742971  9.02839659 ... 12.92817222 12.93024977\n",
      " 12.93230585]\n",
      "np.squeeze(x, axis = 0).shape (1036,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO - 2022-11-11 05:59:45,511] Number of shots of interest: 9\n",
      "[INFO - 2022-11-11 05:59:45,513] Extracting 9 sources\n",
      "[INFO - 2022-11-11 05:59:45,674] Working on shot: 20180805010\n",
      "[INFO - 2022-11-11 05:59:45,735] Working on shot: 20180806010\n",
      "[INFO - 2022-11-11 05:59:45,792] Working on shot: 20180807008\n",
      "[INFO - 2022-11-11 05:59:45,841] Working on shot: 20180831003\n",
      "[INFO - 2022-11-11 05:59:45,902] Working on shot: 20180904009\n",
      "[INFO - 2022-11-11 05:59:45,962] Working on shot: 20180910008\n",
      "[INFO - 2022-11-11 05:59:46,022] Working on shot: 20200425026\n",
      "[INFO - 2022-11-11 05:59:46,074] Working on shot: 20200502022\n",
      "[INFO - 2022-11-11 05:59:46,131] Working on shot: 20200518022\n",
      "[INFO - 2022-11-11 05:59:47,364] Extracting 1\n",
      "[INFO - 2022-11-11 05:59:47,800] Extraction of sources completed in 0.04 minutes.\n",
      "[INFO - 2022-11-11 05:59:47,924] Retrieved 1 spectra.\n",
      "[INFO - 2022-11-11 05:59:47,991] Finding shots of interest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction['spec'].value [[13.62506742 13.61212344 13.59916619 ... 34.87492323 34.87145585\n",
      "  34.86802263]]\n",
      "extraction['spec'].value.shape (1, 1036)\n",
      "x [[13.62506742 13.61212344 13.59916619 ... 34.87492323 34.87145585\n",
      "  34.86802263]]\n",
      "x[0].shape (1036,)\n",
      "x.shape (1, 1036)\n",
      "np.squeeze(x, axis = 0) [13.62506742 13.61212344 13.59916619 ... 34.87492323 34.87145585\n",
      " 34.86802263]\n",
      "np.squeeze(x, axis = 0).shape (1036,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO - 2022-11-11 05:59:53,953] Number of shots of interest: 4\n",
      "[INFO - 2022-11-11 05:59:53,954] Extracting 4 sources\n",
      "[INFO - 2022-11-11 05:59:54,118] Working on shot: 20210608012\n",
      "[INFO - 2022-11-11 05:59:54,173] Working on shot: 20210609014\n",
      "[INFO - 2022-11-11 05:59:54,231] Working on shot: 20210611012\n",
      "[INFO - 2022-11-11 05:59:54,285] Working on shot: 20210612013\n",
      "[INFO - 2022-11-11 05:59:55,341] Extracting 1\n",
      "[INFO - 2022-11-11 05:59:55,721] Extraction of sources completed in 0.03 minutes.\n",
      "[INFO - 2022-11-11 05:59:55,830] Retrieved 1 spectra.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction['spec'].value [[ 3.98050247  3.98241521  3.98431945 ... 14.13570146 14.13704473\n",
      "  14.13837403]]\n",
      "extraction['spec'].value.shape (1, 1036)\n",
      "x [[ 3.98050247  3.98241521  3.98431945 ... 14.13570146 14.13704473\n",
      "  14.13837403]]\n",
      "x[0].shape (1036,)\n",
      "x.shape (1, 1036)\n",
      "np.squeeze(x, axis = 0) [ 3.98050247  3.98241521  3.98431945 ... 14.13570146 14.13704473\n",
      " 14.13837403]\n",
      "np.squeeze(x, axis = 0).shape (1036,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_nep_LS = []\n",
    "\n",
    "#random_nep_spectra = np.zeros(0)\n",
    "\n",
    "for i in range(3):\n",
    "    #pick a random coordinate from the skycoord object\n",
    "    random_coord = hdr3_skycoords[random.randint(0, hdr3_skycoords.size)]\n",
    "    # extract spectra at this random coordinate and append to list, can't append to np array because of \n",
    "    # data type issues will convert to array after loop!\n",
    "    extraction = get_spectra(random_coord)\n",
    "    x = extraction['spec'].value\n",
    "    print(\"extraction['spec'].value\", extraction['spec'].value)\n",
    "    print(\"extraction['spec'].value.shape\", extraction['spec'].value.shape)\n",
    "    print('x', x)\n",
    "    print(\"x[0].shape\", x[0].shape)\n",
    "    #print(\"np.squeeze(x, axis = 0)\", np.squeeze(x, axis = 0))\n",
    "    print(\"x.shape\", x.shape)\n",
    "    print(\"np.squeeze(x, axis = 0)\", np.squeeze(x, axis = 0))\n",
    "    print(\"np.squeeze(x, axis = 0).shape\", np.squeeze(x, axis = 0).shape)\n",
    "    print()\n",
    "    #random_nep_LS.append(extraction['spec'].value)\n",
    "    random_nep_LS.append(np.squeeze(extraction['spec'].value, axis = 0))\n",
    "    #random_nep_spectra = np.append(random_nep_spectra, np.array([extraction['spec'].value]))\n",
    "    \n",
    "random_nep_spectra = np.array(random_nep_LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f5713a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1036)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_nep_spectra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dddc4c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72292c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1036)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_nep_spectra[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44c80430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random_nep_spectra[0]\n",
    "np.squeeze(x, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "70443ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[3.3869852 , 3.3821485 , 3.37731035, ..., 4.68159778, 4.6813685 ,\n",
       "               4.68114122]])                                                   ,\n",
       "       array([[3.11184118, 3.11161769, 3.11138568, ..., 4.64897883, 4.64893582,\n",
       "               4.64889315]])                                                   ,\n",
       "       array([[2.03761393, 2.0388118 , 2.04000585, ..., 7.82602145, 7.82574789,\n",
       "               7.82547692],\n",
       "              [1.22775192, 1.22895395, 1.23014933, ..., 5.89813746, 5.89783571,\n",
       "               5.8975368 ]])                                                   ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_nep_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3d716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = random_nep_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9664ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[3.3869852 , 3.3821485 , 3.37731035, ..., 4.68159778, 4.6813685 ,\n",
       "               4.68114122]])                                                   ,\n",
       "       array([[3.11184118, 3.11161769, 3.11138568, ..., 4.64897883, 4.64893582,\n",
       "               4.64889315]])                                                   ,\n",
       "       array([[2.03761393, 2.0388118 , 2.04000585, ..., 7.82602145, 7.82574789,\n",
       "               7.82547692],\n",
       "              [1.22775192, 1.22895395, 1.23014933, ..., 5.89813746, 5.89783571,\n",
       "               5.8975368 ]])                                                   ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4207b555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a562df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e720c9c",
   "metadata": {},
   "source": [
    "Current problem: working on getting the array to a normal shape!\n",
    "\n",
    "FIGURED IT OUT! Just need to use np.squeeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdf9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in spectra samples\n",
    "star_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/star_spec_reduced.npy\") \n",
    "agn_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/all_agn_no_duplicates.npy\")\n",
    "lowz_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/lowz_no_duplicates.npy\")\n",
    "gal_spec = np.load(\"/home/jovyan/work/stampede2/Nick_Capstone_Project/Valentina_samples/gal_sample.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agn_spec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08939c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(star_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks for nan values.\n",
    "# Interestingly enough, when you do nan != nan, you get True. Becuase NaN is unequal with anything.\n",
    "# But with numbers, you'll get false.\n",
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73500a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any values in the imports that are 'nan' (Not a Number) we remove them.\n",
    "# Go through each value in matrix and check if NaN\n",
    "def remove_nan(file):\n",
    "    for i in range(0,len(file)):\n",
    "        for j in range(0,len(file[i])):\n",
    "            if isNaN(file[i][j]):\n",
    "                # if value is nan, make the value equal to 0.0001\n",
    "                # I'm assuming this was to avoid errors\n",
    "                file[i][j]=0.00001\n",
    "\n",
    "remove_nan(star_spec)\n",
    "remove_nan(agn_spec)\n",
    "remove_nan(lowz_spec)\n",
    "remove_nan(gal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(2) will make the random numbers predictable, so random numbers, but the same random numbers from the same set/seed\n",
    "# so I'll get the same random numbers everytime, until I change the seed\n",
    "np.random.seed(2)\n",
    "# random.shuffle will shuffle the values in an array or list. Like shuffling a deck of cards\n",
    "np.random.shuffle(star_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate just joins arrays\n",
    "all_images = np.concatenate((star_spec[::90],agn_spec,lowz_spec,gal_spec))\n",
    "# making a training set\n",
    "train_images = np.concatenate((star_spec[::90][:433],agn_spec[:444],lowz_spec[:837],gal_spec[:254]))\n",
    "#train_images = agn_spec[:433]\n",
    "#np.random.shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure what these val_images are, it seems to be the opposite half of the train_images.\n",
    "val_images = np.concatenate((star_spec[::90][433:],agn_spec[444:],lowz_spec[837:],gal_spec[254:]))\n",
    "#val_images = agn_spec[433:]\n",
    "#np.random.shuffle(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will normalize the data.\n",
    "# The idea is, there is such a large range of sources so it may be simpler for the algorithm to recognize sources\n",
    "# if they are between 0 and 1.\n",
    "# Dividing by the largest number preserves the information but scales it down.\n",
    "# If we don't do this the sources could go from a very large number to a small one, which makes it harder for the algorithm\n",
    "def Norm(data):\n",
    "    new_data = []\n",
    "    for spec in data:\n",
    "        #print('spec_before', spec)\n",
    "        spec = spec/spec.max()\n",
    "        #print('spec_after', spec)\n",
    "        #print('spec.max', spec.max())\n",
    "        new_data.append(spec)\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing both data sets\n",
    "train_images = Norm(train_images)\n",
    "val_images = Norm(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c794651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making cuts of the image sets and then normalizing them.\n",
    "train_cut = train_images[:,88:1002]\n",
    "val_cut = val_images[:,88:1002]\n",
    "train_cut = Norm(train_cut)\n",
    "val_cut = Norm(val_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b19cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 30\n",
    "act = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      #layers.InputLayer(input_shape=(1036)),\n",
    "      layers.Dense(436, activation=act,name=\"e1\"),\n",
    "      #layers.Dropout(0.3),\n",
    "      #layers.Dense(136, activation=act, name ='e2'),\n",
    "      #BatchNormalization(),\n",
    "      layers.Dropout(0.3),\n",
    "      layers.Dense(latent_dim, activation=None, name = \"e3\"),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      #layers.InputLayer(input_shape=(latent_dim,)),\n",
    "      #layers.Dense(136, activation=act,name = \"d1\"),\n",
    "      layers.Dense(436, activation=act,name = \"d2\"),\n",
    "      #BatchNormalization(),\n",
    "      layers.Dense(914, activation=None,name = \"d3\")\n",
    "    ])#914 for cut. 1036 for all\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a658af",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_dim)\n",
    "#autoencoder.build(input_shape=np.shape(train_images))\n",
    "epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adg = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    d = tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)\n",
    "    #tf.print(d, [d], \"Inside loss function\")\n",
    "    #tf.print(d.shape)\n",
    "    return d\n",
    "\n",
    "autoencoder.compile(optimizer=adam, loss=loss1)#es.MeanSquaredError())\n",
    "\n",
    "try:\n",
    "    os.mkdir(f\"models/training{training_index}\")\n",
    "    checkpoint_path = f\"models/training{training_index}/cp_{training_index}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "except:\n",
    "    checkpoint_path = f\"models/training{training_index}/cp_{training_index}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458d6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "#autoencoder.summary()\n",
    "#autoencoder.fit(train_images, train_images,\n",
    " #               epochs=10,\n",
    "  #              shuffle=True,\n",
    "   #             validation_data=(val_images,val_images))\n",
    "hist = autoencoder.fit(train_cut, train_cut,\n",
    "                epochs=epoch,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_cut,val_cut),\n",
    "                callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b415d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.linspace(1,epoch+1,epoch)\n",
    "plt.plot(x,hist.history[\"loss\"],label=\"loss\")\n",
    "plt.plot(x,hist.history[\"val_loss\"],label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'Graphs/loss_{training_index}.png')\n",
    "print(np.shape(train_images))\n",
    "\n",
    "encoded_imgs = autoencoder.encoder(train_cut).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "np.save(f\"encoded_{training_index}\",encoded_imgs)\n",
    "np.save(f\"decoded_{training_index}\",decoded_imgs)\n",
    "\n",
    "encoded_val = autoencoder.encoder(val_cut).numpy()\n",
    "np.save(f\"encoded_val_{training_index}\",encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b73022",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"encoded_31_cut.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_cut[4])\n",
    "plt.savefig(\"OG.png\")\n",
    "plt.figure()\n",
    "plt.plot(decoded_imgs[4])\n",
    "plt.savefig(\"RECON.png\")\n",
    "print(hist.history[\"val_loss\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fb812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
