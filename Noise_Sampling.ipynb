{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48926f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means that my matplotlib graphs will be included in the notebook, next to the code\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import astropy\n",
    "import random\n",
    "import numpy as np\n",
    "import tables as tb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from astropy.table import Table, Column, join\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "\n",
    "from hetdex_api.config import HDRconfig\n",
    "from hetdex_api.detections import Detections\n",
    "from hetdex_api.elixer_widget_cls import ElixerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b25f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why the code below is here, it was in the Detections database and API notebook\n",
    "# https://github.com/HETDEX/hetdex_api/blob/master/notebooks/api-notebooks/03-Detections_Database_and_API.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce894e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88dea4",
   "metadata": {},
   "source": [
    "### Opens the catalogs and turns them into dataframes\n",
    "\n",
    "I like to open both catalogs separately since they are both big (HDR3 especially!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cde6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening H20 NEP catalog and converting it into a pandas DF\n",
    "H20_NEP_catalog = fits.open('H20_NEP_VIRUS_OVERLAP_CAT_10_2021.fits', memmap = True)\n",
    "H20_NEP_data = H20_NEP_catalog[1].data\n",
    "H20_NEP_DF = pd.DataFrame(H20_NEP_data, columns=H20_NEP_data.columns.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c27a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening HDR3 detections catalog  ** double check this <-- statement ** and converting it into a pandas DF\n",
    "HDR_source_cat = fits.open('/home/jovyan/Hobby-Eberly-Telesco/hdr3/catalogs/source_catalog_3.0.1.fits', memmap = True)\n",
    "HDR3_data = HDR_source_cat[1].data\n",
    "HDR3_DF = pd.DataFrame(HDR3_data, columns=HDR3_data.columns.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87243526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we will then take from the entire data set (it was huge so we needed to determine what we wanted to look at specifically).\n",
    "# As the name suggests, these are the ones that are useful to us!\n",
    "useful_hdr3_cols = ['source_id', 'detectid',  'selected_det', 'ra_mean', 'dec_mean', 'fwhm', 'shotid', 'field',  'ra', 'dec', 'wave', 'wave_err', 'flux', 'flux_err', 'sn', 'sn_err', 'chi2', 'chi2_err',\n",
    "'linewidth', 'linewidth_err', 'plya_classification', 'z_hetdex', 'z_hetdex_conf', 'combined_plae']\n",
    "\n",
    "# For now, the only useful columns for us in H20 NEP is RA and DEC.\n",
    "useful_h20nep_cols = ['RA_MODELING', 'DEC_MODELING', 'VALID_SOURCE_MODELING']\n",
    "\n",
    "# From the original DF, taking the useful columns\n",
    "reduced_hdr3_df = HDR3_DF.loc[:, useful_hdr3_cols]\n",
    "reduced_h20nep_df = H20_NEP_DF.loc[:, useful_h20nep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ba3f0",
   "metadata": {},
   "source": [
    "### Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44649c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data from before 2017 because it isn't good (not useful to us)\n",
    "# No need to do this for H20 NEP\n",
    "removed_bad_shots_hdr3_df = reduced_hdr3_df[reduced_hdr3_df.shotid.values >= 20180000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a97cd9",
   "metadata": {},
   "source": [
    "## Question: Meaning of the VALID_SOURCE_MODELING column?\n",
    "\n",
    "\n",
    "How do I apply this sn filter to the H20 catalog? Is there a signal to noise column I'm missing?\n",
    "\n",
    "No need! For H20 we wont use any filters aside from the the VALID_SOURCE_MODELING column which just tells us that the model was able to converge and get fluxes from the source.\n",
    "\n",
    "Nah the False valid source modeling means that the model used to measure the fluxes failed somehow so we cannot use that galaxy reliably\n",
    "\n",
    "We want the true ones since we know the model was able to find a galaxy and we can use that for our imaging counterpart identification (ie: to use these galaxy to check if there is a galaxy at our new extraction coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9e5f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give high confidence detections. Something we would want to do also. What is sn threshold that Valentina's code is having trouble with.\n",
    "# Reason why, we want high-confidence Lya. If we are very confident sn and another filter, then that's what we consider high-conf lya.\n",
    "# Once noise and high-confidence sample. We can start exanping on valentina's code and do our own stuff\n",
    "signal_to_noise_interval = removed_bad_shots_hdr3_df[removed_bad_shots_hdr3_df['sn'] > 6.5]\n",
    "\n",
    "# For now, no need to specify a field. But once trained, we want to run this for the NEP field!\n",
    "\n",
    "valid_source_check = reduced_h20nep_df[reduced_h20nep_df['VALID_SOURCE_MODELING'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467aa59",
   "metadata": {},
   "source": [
    "### Picking a random source and applying an offset to that source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a489c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks a random source for us, the 1 means a single random source.\n",
    "# Might need to find a different way of getting a random source, just because I don't think\n",
    "# there's a way to control which source this gets, so it'll always be a different source!\n",
    "# Or just run this once\n",
    "random_source = signal_to_noise_interval.sample(n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "99e57c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using an offset to try to pick an area where there is no source.\n",
    "# This is a value we are experimenting with, going to go with 20 arcseconds for now\n",
    "offset = 20 * u.arcsec\n",
    "# need to convert to degrees so I can add to the ra and dec in catalog,\n",
    "# the ra and dec in catalog are in degrees\n",
    "offset = offset.to('deg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "38f35710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying an offset, then we check if there is a source at the offset!\n",
    "delta_ra = random_source['ra'] + offset # I forgot how units work here, are these all in arcseconds?\n",
    "delta_dec = random_source['dec'] + offset # I forgot how units work here, are these all in arcseconds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4f7e4",
   "metadata": {},
   "source": [
    "## Should I make this check an or?\n",
    "Because this is saying, if the ra AND the dec are not in the catalog, then there is no source.\n",
    "\n",
    "But if only one isn't there then it's possible the source isn't in the catalog either right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f1c0a",
   "metadata": {},
   "source": [
    "Here I'm checking if the source in the HDR3 detections catalog? Ask about the catalog name because I think I'm getting mixed up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f6572",
   "metadata": {},
   "source": [
    "When comparing the H20 NEP values to the running into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f0ae9",
   "metadata": {},
   "source": [
    "### Checking if the offset ra and dec are in either of the catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e113ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This truth_check df will check if the delta_ra and delta_dec are in the catalog. \n",
    "# If the size of this df is 0, then there is no source in this catalog with those specific ra and dec.\n",
    "truth_check_hdr3 = signal_to_noise_interval[(signal_to_noise_interval['ra'] == delta_ra.values[0])  \n",
    "                                            & (signal_to_noise_interval['dec'] == delta_dec.values[0])]\n",
    "truth_check_h20 = reduced_h20nep_df[(np.isclose(reduced_h20nep_df['RA_MODELING'], delta_ra.values[0], 1e-10, 1e-11))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "05629b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This truth_check df will check if the delta_ra and delta_dec are in the catalog. \n",
    "# If the size of this df is 0, then there is no source in this catalog with those specific ra and dec.\n",
    "truth_check_hdr3 = signal_to_noise_interval[(signal_to_noise_interval['ra'] == delta_ra.values[0])  \n",
    "                                            & (signal_to_noise_interval['dec'] == delta_dec.values[0])]\n",
    "\n",
    "# For this truth check I have to use np.isclose. The reason is, because the RA and DEC in the HDR3 catalog \n",
    "# values only go to 7 decimals and the h20 catalog numbers go to 14 decimals. So I can't check for equality\n",
    "# normality since I'll always get not equal. If I use close I can check if the numbers are close with a \n",
    "# certain chosen error.\n",
    "\n",
    "# If later on samples seem weirdly small, I need to come back and check this!!!!!!*********\n",
    "truth_check_h20 = valid_source_check[(np.isclose(valid_source_check['RA_MODELING'], delta_ra.values[0], 1e-9, 1e-10))\n",
    "                                   & (np.isclose(valid_source_check['DEC_MODELING'], delta_dec.values[0], 1e-9, 1e-10))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ad73e825",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_check_hdr3.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5e2d9a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth_check_h20.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c20ad6",
   "metadata": {},
   "source": [
    "We want no source. This catalog has HETDEX detections one. \n",
    "\n",
    "    Want to make sure:\n",
    "        1.No hetdection detec\n",
    "        \n",
    "        2. No imaging counterpart. Do some cross-matching. Gives us a 0 and THEN we extract. Want to extract in basically empty space. Start with 100. \n",
    "        \n",
    "            Coordinates still. Trying to see if no match with the .fits file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330499d",
   "metadata": {},
   "source": [
    "Start with detection. One approach was fits file with coordinates. \n",
    "\n",
    "Or \n",
    "\n",
    "Use this but expand upon it. Find RA and DEC of each shot. And randomly extract.\n",
    "\n",
    "    delta ra and delta dec. Double check if is there a source there. \n",
    "    \n",
    "For noise sample, no need to run through valentina's code. Only focus on High-z after filtering through Valentina's code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0dae8",
   "metadata": {},
   "source": [
    "Once we have noise sample.\n",
    "\n",
    "Run through valentina's code. Hopefully it detects them all as high-z. Cause neither low-z or star.\n",
    "\n",
    "Two skycoords. Check coordinates to see if HETDEX detection is there. Compare minimum separation. If the difference is smaller than 3 arcseconds. Then there is a source there, so do not extract there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
